{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Final Project for DS2002 - By Gabriel Jacksnon (tbp8gx)\n",
    "\n",
    "## Summary:\n",
    " - I'm using adventureworks as the source dataset. I'm using a schema similar to the schema from the midterm project.\n",
    " - Schema:\n",
    "    - DimCustomers - Read from MySQL\n",
    "    - DimDates - Read from MySQL\n",
    "    - DimProducts - Read from MongoDB atlas (stored on the cloud's file storage)\n",
    "    - DimTerritories - Read from MongoDB atlas (stored on the cloud's file storage)\n",
    "    - FactSaleOrders - 3 files of data that are streamed locally \n",
    "\n",
    "## Design Requirements:\n",
    "- I have three dimension tables (dimCustomers, dimProducts, dimTerritories).\n",
    "- I use the date dimension (dimDate).\n",
    "- I use FactSaleOrders as my fact table that models sales.\n",
    "- My solution uses dimensions from various sources\n",
    "    1) I use MySQL for the relational database\n",
    "    2) I use Mongo for the NoSQL database\n",
    "    3) I use MongoDB Atlas to pull files from a cloud-based file system. \n",
    "- I use static data for the dimension data and dynamic data for the fact data, which can be seen in the data/adventureworks/streaming directory.\n",
    "- I execute a SELECT query to summarize the top 10 buying customers in the US territory.\n",
    "\n",
    "## Functional Requirements:\n",
    "- I use 1 batch execution from the data/adventureworks/streaming files.\n",
    "- I use Spark AutoLoader to load fact sales source data from data/adventureworks/streaming location with sales1.json, sales2.json, and sales3.json as the 3 intervals of source data.\n",
    "- I implement this with bronze, silver, and gold-level architecture.\n",
    "\n",
    "## Procedure:\n",
    "1. Set up environment to connect to MySQL and MongoDB. Initialize Data Lakehouse files.\n",
    "2. Load (static) dimensions\n",
    "3. Perform bronze-level actions\n",
    "4. Perform silver-level actions\n",
    "5. Perform gold-level actions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce389adf724eac82"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 1 - SET UP ENVIRONMENT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36c0206e13644785"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:04:55.428412Z",
     "start_time": "2025-05-09T21:04:54.867649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SQL Alchemy Version: 2.0.40\n",
      "Running PyMongo Version: 4.12.1\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Handle imports/Runtime issues\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy\n",
    "import datetime\n",
    "import certifi\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import pymongo\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "print(f\"Running SQL Alchemy Version: {sqlalchemy.__version__}\")\n",
    "print(f\"Running PyMongo Version: {pymongo.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 1.2 Specify connection and initialization information\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify MySQL Server Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mysql_args = {\n",
    "    \"host_name\" : \"localhost\",\n",
    "    \"port\" : \"3306\",\n",
    "    \"db_name\" : \"adventureworks\",\n",
    "    \"conn_props\" : {\n",
    "        \"user\" : \"root\",\n",
    "        \"password\" : \"password\",\n",
    "        \"driver\" : \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify MongoDB Cluster Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mongodb_args = {\n",
    "    \"user_name\" : \"Cluster94547\",\n",
    "    \"password\" : \"cWdKdENBS2pj\",\n",
    "    \"cluster_name\" : \"Cluster94547\",\n",
    "    \"cluster_subnet\" : \"hp0v9\",\n",
    "    \"cluster_location\" : \"atlas\", # \"local\"\n",
    "    \"db_name\" : \"final\"\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'data')\n",
    "data_dir = os.path.join(base_dir, 'adventureworks')\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"adventureworks_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "trans_output_bronze = os.path.join(database_dir, 'fact_trans', 'bronze')\n",
    "trans_output_silver = os.path.join(database_dir, 'fact_trans', 'silver')\n",
    "trans_output_gold = os.path.join(database_dir, 'fact_trans', 'gold')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:04:56.208031Z",
     "start_time": "2025-05-09T21:04:56.202573Z"
    }
   },
   "id": "b3996f8cd4074ec"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 1.3 Specify helper functions (provided from the class)\n",
    "\n",
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "\n",
    "    '''Fetch each item in the directory, and filter out any directories.'''\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))])\n",
    "\n",
    "    '''Populate lists with the Size and Last Modification DateTime for each file in the directory.'''\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "\n",
    "    return pd.DataFrame(data=data, columns=column_names)\n",
    "\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "\n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "\n",
    "def drop_null_columns(df, threshold):\n",
    "    '''Drop Columns having a percentage of NULL values that exceeds the given 'threshold' parameter value.'''\n",
    "    columns_with_nulls = [col for col in df.columns if df.filter(df[col].isNull()).count() / df.count() > threshold]\n",
    "    df_dropped = df.drop(*columns_with_nulls)\n",
    "\n",
    "    return df_dropped\n",
    "\n",
    "\n",
    "def get_mysql_dataframe(spark_session, sql_query : str, **args):\n",
    "    '''Create a JDBC URL to the MySQL Database'''\n",
    "    jdbc_url = f\"jdbc:mysql://{args['host_name']}:{args['port']}/{args['db_name']}\"\n",
    "\n",
    "    '''Invoke the spark.read.format(\"jdbc\") function to query the database, and fill a DataFrame.'''\n",
    "    dframe = spark_session.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"driver\", args['conn_props']['driver']) \\\n",
    "        .option(\"user\", args['conn_props']['user']) \\\n",
    "        .option(\"password\", args['conn_props']['password']) \\\n",
    "        .option(\"query\", sql_query) \\\n",
    "        .load()\n",
    "\n",
    "    return dframe\n",
    "\n",
    "\n",
    "def get_mongo_uri(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the 'cluster_location' parameter.\")\n",
    "\n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "    return uri\n",
    "\n",
    "\n",
    "def get_spark_conf_args(spark_jars : list, **args):\n",
    "    jars = \"\"\n",
    "    for jar in spark_jars:\n",
    "        jars += f\"{jar}, \"\n",
    "\n",
    "    sparkConf_args = {\n",
    "        \"app_name\" : \"PySpark Northwind Data Lakehouse (Medallion Architecture)\",\n",
    "        \"worker_threads\" : f\"local[{int(os.cpu_count()/2)}]\",\n",
    "        \"shuffle_partitions\" : int(os.cpu_count()),\n",
    "        \"mongo_uri\" : get_mongo_uri(**args),\n",
    "        \"spark_jars\" : jars[0:-2],\n",
    "        \"database_dir\" : sql_warehouse_dir\n",
    "    }\n",
    "\n",
    "    return sparkConf_args\n",
    "\n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name']) \\\n",
    "        .setMaster(args['worker_threads']) \\\n",
    "        .set('spark.driver.memory', '4g') \\\n",
    "        .set('spark.executor.memory', '2g') \\\n",
    "        .set('spark.jars', args['spark_jars']) \\\n",
    "        .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "        .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "        .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "        .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "        .set('spark.sql.debug.maxToStringFields', 35) \\\n",
    "        .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "        .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "        .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "        .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "        .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "\n",
    "    return sparkConf\n",
    "\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Get MongoDB Client Connection'''\n",
    "    mongo_uri = get_mongo_uri(**args)\n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        client = pymongo.MongoClient(mongo_uri, tlsCAFile=certifi.where())\n",
    "\n",
    "    elif args['cluster_location'] == \"local\":\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"A MongoDB Client could not be created.\")\n",
    "\n",
    "    return client\n",
    "\n",
    "\n",
    "def set_mongo_collections(mongo_client, db_name : str, data_directory : str, json_files : list):\n",
    "    db = mongo_client[db_name]\n",
    "\n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "\n",
    "    mongo_client.close()\n",
    "\n",
    "\n",
    "def get_mongodb_dataframe(spark_session, **args):\n",
    "    '''Query MongoDB, and create a DataFrame'''\n",
    "    dframe = spark_session.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .option(\"database\", args['db_name']) \\\n",
    "        .option(\"collection\", args['collection']).load()\n",
    "\n",
    "    '''Drop the '_id' index column to clean up the response.'''\n",
    "    dframe = dframe.drop('_id')\n",
    "\n",
    "    '''Call the drop_null_columns() function passing in the dataframe.'''\n",
    "    dframe = drop_null_columns(dframe, args['null_column_threshold'])\n",
    "\n",
    "    return dframe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:04:58.102290Z",
     "start_time": "2025-05-09T21:04:58.093911Z"
    }
   },
   "id": "7e4bfec93f06bc03"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 1.4 Populate MongoDB with source data (products.json, territories.json)\n",
    "\n",
    "client = get_mongo_client(**mongodb_args)\n",
    "\n",
    "json_files = {\n",
    "    \"products\": \"products.json\",\n",
    "    \"territories\": \"territories.json\"\n",
    "}\n",
    "\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], batch_dir, json_files)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:05:01.454185Z",
     "start_time": "2025-05-09T21:04:58.974815Z"
    }
   },
   "id": "7323103b7c5533c3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Directory '/Users/gabooj/IdeaProjects/final-ds/spark-warehouse/adventureworks_dlh.db' has been removed successfully.\""
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.5 Remove any data in data lakehouse directory\n",
    "remove_directory_tree(database_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:05:02.663825Z",
     "start_time": "2025-05-09T21:05:02.654673Z"
    }
   },
   "id": "3eb1bf917552e8a4"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/09 17:05:06 WARN Utils: Your hostname, Gabes-iMac.local resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface en1)\n",
      "25/05/09 17:05:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/gabooj/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/gabooj/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8ade8d8f-4afc-4acc-96e6-a60e745d88b9;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/gabooj/IdeaProjects/final-ds/venv/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 113ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8ade8d8f-4afc-4acc-96e6-a60e745d88b9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n",
      "25/05/09 17:05:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x122ecbb50>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://192.168.1.29:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.5</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySpark Northwind Data Lakehouse (Medallion Architecture)</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.6 Create a new spark session\n",
    "\n",
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "\n",
    "jars = []\n",
    "mysql_spark_jar = os.path.join(os.getcwd(), \"mysql-connector-j-9.3.0.jar\")\n",
    "jars.append(mysql_spark_jar)\n",
    "\n",
    "sparkConf_args = get_spark_conf_args(jars, **mongodb_args)\n",
    "\n",
    "sparkConf = get_spark_conf(**sparkConf_args)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "spark"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:05:08.859138Z",
     "start_time": "2025-05-09T21:05:05.605152Z"
    }
   },
   "id": "89b0f54824c0a599"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.7 Create a new metadata database\n",
    "\n",
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE;\")\n",
    "\n",
    "sql_create_db = f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "    COMMENT 'DS-2002 Final Database'\n",
    "    WITH DBPROPERTIES (contains_pii = true, purpose = 'DS-2002 Final');\n",
    "\"\"\"\n",
    "spark.sql(sql_create_db)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:05:12.059767Z",
     "start_time": "2025-05-09T21:05:11.010071Z"
    }
   },
   "id": "3f5ee434b9c58954"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 2. LOAD (STATIC/\"COLD PATH\") DIMENSIONS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52b5034b92c8277e"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "   date_key   full_date    date_name\n0  20000101  2000-01-01  2000/01/01 \n1  20000102  2000-01-02  2000/01/02 ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_key</th>\n      <th>full_date</th>\n      <th>date_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20000101</td>\n      <td>2000-01-01</td>\n      <td>2000/01/01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000102</td>\n      <td>2000-01-02</td>\n      <td>2000/01/02</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.1 Populate the Date Dimension\n",
    "sql_dim_date = f\"SELECT date_key, full_date, date_name FROM {mysql_args['db_name']}.dim_date\"\n",
    "df_dim_date = get_mysql_dataframe(spark, sql_dim_date, **mysql_args)\n",
    "\n",
    "# 2.2 Save the Date Dimension\n",
    "df_dim_date.write.saveAsTable(f\"{dest_database}.dim_date\", mode=\"overwrite\")\n",
    "df_dim_date.toPandas().head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:05:17.327542Z",
     "start_time": "2025-05-09T21:05:15.016150Z"
    }
   },
   "id": "90bc3936e711e7f5"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "   CustomerID FirstName LastName EmailAddress Phone AccountNumber  \\\n0           1      None     None         None  None    AW00000001   \n1           2      None     None         None  None    AW00000002   \n\n  CustomerType        AddressLine1 AddressLine2     City PostalCode  \\\n0            S  2251 Elliot Avenue         None  Seattle      98104   \n1            S     7943 Walnut Ave         None   Renton      98055   \n\n  StateProvince        Country  \n0    Washington  United States  \n1    Washington  United States  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CustomerID</th>\n      <th>FirstName</th>\n      <th>LastName</th>\n      <th>EmailAddress</th>\n      <th>Phone</th>\n      <th>AccountNumber</th>\n      <th>CustomerType</th>\n      <th>AddressLine1</th>\n      <th>AddressLine2</th>\n      <th>City</th>\n      <th>PostalCode</th>\n      <th>StateProvince</th>\n      <th>Country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>AW00000001</td>\n      <td>S</td>\n      <td>2251 Elliot Avenue</td>\n      <td>None</td>\n      <td>Seattle</td>\n      <td>98104</td>\n      <td>Washington</td>\n      <td>United States</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>AW00000002</td>\n      <td>S</td>\n      <td>7943 Walnut Ave</td>\n      <td>None</td>\n      <td>Renton</td>\n      <td>98055</td>\n      <td>Washington</td>\n      <td>United States</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.3 Populate the Customers Dimension by executing a MySQL query. This uses joins to collect information from the 'adventureworks' DB.\n",
    "sql_customers = \"\"\"\n",
    "    SELECT \n",
    "        c.CustomerID,\n",
    "        contact.FirstName,\n",
    "        contact.LastName,\n",
    "        contact.EmailAddress,\n",
    "        contact.Phone,\n",
    "        c.AccountNumber,\n",
    "        c.CustomerType,\n",
    "        a.AddressLine1,\n",
    "        a.AddressLine2,\n",
    "        a.City,\n",
    "        a.PostalCode,\n",
    "        sp.Name AS StateProvince,\n",
    "        cr.Name AS Country\n",
    "    FROM adventureworks.customer AS c\n",
    "    LEFT OUTER JOIN customeraddress AS ca\n",
    "    ON c.CustomerID = ca.CustomerID\n",
    "    LEFT OUTER JOIN address AS a \n",
    "    ON ca.AddressID = a.AddressID\n",
    "    LEFT OUTER JOIN addresstype AS t\n",
    "\tON ca.AddressTypeID = t.AddressTypeID\n",
    "\tLEFT OUTER JOIN stateprovince AS sp\n",
    "\tON sp.StateProvinceID = a.StateProvinceID\n",
    "\tLEFT OUTER JOIN countryregion AS cr\n",
    "\tON sp.CountryRegionCode = cr.CountryRegionCode\n",
    "\tLEFT OUTER JOIN individual AS i\n",
    "\tON c.CustomerID = i.CustomerID\n",
    "\tLEFT OUTER JOIN contact\n",
    "\tON i.ContactID = contact.ContactID\n",
    "\"\"\"\n",
    "df_dim_customers = get_mysql_dataframe(spark, sql_customers, **mysql_args)\n",
    "df_dim_customers.toPandas().head(2)\n",
    "\n",
    "# ***** Please note that the first customer records do not have all of their information,\n",
    "# ***** which is why the head of the table (below) is missing some information."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:05:18.850416Z",
     "start_time": "2025-05-09T21:05:18.310624Z"
    }
   },
   "id": "b67df7de166b26aa"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "   CustomerID FirstName LastName EmailAddress Phone AccountNumber  \\\n0           1      None     None         None  None    AW00000001   \n1           3      None     None         None  None    AW00000003   \n\n  CustomerType           AddressLine1 AddressLine2     City PostalCode  \\\n0            S     2251 Elliot Avenue         None  Seattle      98104   \n1            S  12345 Sterling Avenue         None   Irving      75061   \n\n  StateProvince        Country  CustomerKey  \n0    Washington  United States            1  \n1         Texas  United States            1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CustomerID</th>\n      <th>FirstName</th>\n      <th>LastName</th>\n      <th>EmailAddress</th>\n      <th>Phone</th>\n      <th>AccountNumber</th>\n      <th>CustomerType</th>\n      <th>AddressLine1</th>\n      <th>AddressLine2</th>\n      <th>City</th>\n      <th>PostalCode</th>\n      <th>StateProvince</th>\n      <th>Country</th>\n      <th>CustomerKey</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>AW00000001</td>\n      <td>S</td>\n      <td>2251 Elliot Avenue</td>\n      <td>None</td>\n      <td>Seattle</td>\n      <td>98104</td>\n      <td>Washington</td>\n      <td>United States</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>AW00000003</td>\n      <td>S</td>\n      <td>12345 Sterling Avenue</td>\n      <td>None</td>\n      <td>Irving</td>\n      <td>75061</td>\n      <td>Texas</td>\n      <td>United States</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.4 Transform df_dim_customers (add Key from temp view)\n",
    "df_dim_customers.createOrReplaceTempView(\"customers\")\n",
    "sql_customers = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY CustomerID ORDER BY CustomerID) AS CustomerKey\n",
    "    FROM customers;\n",
    "\"\"\"\n",
    "df_dim_customers = spark.sql(sql_customers)\n",
    "\n",
    "# 2.5 Save df_dim_customers\n",
    "df_dim_customers.write.saveAsTable(f\"{dest_database}.dim_customers\", mode=\"overwrite\")\n",
    "\n",
    "# 2.6 Demonstrate that df_dim_customers exists in Spark\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_customers LIMIT 2\").toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:05:21.197086Z",
     "start_time": "2025-05-09T21:05:19.871312Z"
    }
   },
   "id": "ce48dde4674929e2"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "   DaysToManufacture FinishedGoodsFlag  ListPrice MakeFlag  \\\n0                  0                 0        0.0        0   \n1                  0                 0        0.0        0   \n2                  1                 0        0.0        1   \n3                  0                 0        0.0        0   \n4                  1                 0        0.0        1   \n\n                    Name  ProductID ProductNumber  ReorderPoint  \\\n0        Adjustable Race          1       AR-5381           750   \n1           Bearing Ball          2       BA-8327           750   \n2        BB Ball Bearing          3       BE-2349           600   \n3  Headset Ball Bearings          4       BE-2908           600   \n4                  Blade        316       BL-2036           600   \n\n   SafetyStockLevel        SellStartDate  StandardCost  \n0              1000  1998-06-01 00:00:00           0.0  \n1              1000  1998-06-01 00:00:00           0.0  \n2               800  1998-06-01 00:00:00           0.0  \n3               800  1998-06-01 00:00:00           0.0  \n4               800  1998-06-01 00:00:00           0.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DaysToManufacture</th>\n      <th>FinishedGoodsFlag</th>\n      <th>ListPrice</th>\n      <th>MakeFlag</th>\n      <th>Name</th>\n      <th>ProductID</th>\n      <th>ProductNumber</th>\n      <th>ReorderPoint</th>\n      <th>SafetyStockLevel</th>\n      <th>SellStartDate</th>\n      <th>StandardCost</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>Adjustable Race</td>\n      <td>1</td>\n      <td>AR-5381</td>\n      <td>750</td>\n      <td>1000</td>\n      <td>1998-06-01 00:00:00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>Bearing Ball</td>\n      <td>2</td>\n      <td>BA-8327</td>\n      <td>750</td>\n      <td>1000</td>\n      <td>1998-06-01 00:00:00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>BB Ball Bearing</td>\n      <td>3</td>\n      <td>BE-2349</td>\n      <td>600</td>\n      <td>800</td>\n      <td>1998-06-01 00:00:00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>Headset Ball Bearings</td>\n      <td>4</td>\n      <td>BE-2908</td>\n      <td>600</td>\n      <td>800</td>\n      <td>1998-06-01 00:00:00</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>Blade</td>\n      <td>316</td>\n      <td>BL-2036</td>\n      <td>600</td>\n      <td>800</td>\n      <td>1998-06-01 00:00:00</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.7 Populate the Products Dimension\n",
    "df_dim_products = get_mongodb_dataframe(spark, db_name=\"final\", collection=\"products\", null_column_threshold=0.4)\n",
    "df_dim_products.toPandas().head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:05:32.124979Z",
     "start_time": "2025-05-09T21:05:22.149177Z"
    }
   },
   "id": "93d6b6f214e76e7c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "   DaysToManufacture FinishedGoodsFlag  ListPrice MakeFlag        Name  \\\n0                  0                 0        0.0        0     Decal 2   \n1                  0                 0        0.0        0  Hex Nut 12   \n\n   ProductID ProductNumber  ReorderPoint  SafetyStockLevel  \\\n0        326       DC-9824           750              1000   \n1        384       HN-3816           750              1000   \n\n         SellStartDate  StandardCost  ProductKey  \n0  1998-06-01 00:00:00           0.0           1  \n1  1998-06-01 00:00:00           0.0           1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DaysToManufacture</th>\n      <th>FinishedGoodsFlag</th>\n      <th>ListPrice</th>\n      <th>MakeFlag</th>\n      <th>Name</th>\n      <th>ProductID</th>\n      <th>ProductNumber</th>\n      <th>ReorderPoint</th>\n      <th>SafetyStockLevel</th>\n      <th>SellStartDate</th>\n      <th>StandardCost</th>\n      <th>ProductKey</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>Decal 2</td>\n      <td>326</td>\n      <td>DC-9824</td>\n      <td>750</td>\n      <td>1000</td>\n      <td>1998-06-01 00:00:00</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>Hex Nut 12</td>\n      <td>384</td>\n      <td>HN-3816</td>\n      <td>750</td>\n      <td>1000</td>\n      <td>1998-06-01 00:00:00</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.8 Transform the df_dim_products (add Key from temp view)\n",
    "df_dim_products.createOrReplaceTempView(\"products\")\n",
    "sql_products = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY ProductID ORDER BY ProductID) AS ProductKey\n",
    "    FROM products;\n",
    "\"\"\"\n",
    "df_dim_products = spark.sql(sql_products)\n",
    "\n",
    "# 2.9 Save df_dim_products\n",
    "df_dim_products.write.saveAsTable(f\"{dest_database}.dim_products\", mode=\"overwrite\")\n",
    "\n",
    "# 2.10 Demonstrate that df_dim_products exists in Spark\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_products LIMIT 2\").toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:05:35.513264Z",
     "start_time": "2025-05-09T21:05:35.016893Z"
    }
   },
   "id": "19c97a9655f2f571"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "  CountryRegionCode          Group  TerritoryID TerritoryName\n0                US  North America            1     Northwest\n1                US  North America            2     Northeast\n2                US  North America            3       Central\n3                US  North America            4     Southwest\n4                US  North America            5     Southeast",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CountryRegionCode</th>\n      <th>Group</th>\n      <th>TerritoryID</th>\n      <th>TerritoryName</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>US</td>\n      <td>North America</td>\n      <td>1</td>\n      <td>Northwest</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>US</td>\n      <td>North America</td>\n      <td>2</td>\n      <td>Northeast</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>US</td>\n      <td>North America</td>\n      <td>3</td>\n      <td>Central</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>US</td>\n      <td>North America</td>\n      <td>4</td>\n      <td>Southwest</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>US</td>\n      <td>North America</td>\n      <td>5</td>\n      <td>Southeast</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.11 Populate the Territories dimension\n",
    "df_dim_territories = get_mongodb_dataframe(spark, db_name=\"final\", collection=\"territories\", null_column_threshold=0.4)\n",
    "df_dim_territories.toPandas().head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:05:37.885587Z",
     "start_time": "2025-05-09T21:05:36.371982Z"
    }
   },
   "id": "b7c277ea607a2ccc"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "  CountryRegionCode          Group  TerritoryID TerritoryName  TerritoryKey\n0                US  North America            1     Northwest             1\n1                US  North America            3       Central             1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CountryRegionCode</th>\n      <th>Group</th>\n      <th>TerritoryID</th>\n      <th>TerritoryName</th>\n      <th>TerritoryKey</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>US</td>\n      <td>North America</td>\n      <td>1</td>\n      <td>Northwest</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>US</td>\n      <td>North America</td>\n      <td>3</td>\n      <td>Central</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.12 Transform the df_dim_territories (add Key from temp view)\n",
    "df_dim_territories.createOrReplaceTempView(\"territories\")\n",
    "sql_territories = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (PARTITION BY TerritoryID ORDER BY TerritoryID) AS TerritoryKey\n",
    "    FROM territories;\n",
    "\"\"\"\n",
    "df_dim_territories = spark.sql(sql_territories)\n",
    "\n",
    "# 2.13 Save df_dim_products\n",
    "df_dim_territories.write.saveAsTable(f\"{dest_database}.dim_territories\", mode=\"overwrite\")\n",
    "\n",
    "# 2.14 Demonstrate that df_dim_products exists in Spark\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_territories LIMIT 2\").toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:05:39.128731Z",
     "start_time": "2025-05-09T21:05:38.787025Z"
    }
   },
   "id": "2196413cf26998bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 3 - BRONZE PHASE (Reading raw data from streamed files)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dbf83734e1e4545"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CarrierTrackingNumber: string (nullable = true)\n",
      " |-- Credit Card ExpMonth: long (nullable = true)\n",
      " |-- Credit Card ExpYear: long (nullable = true)\n",
      " |-- Credit Card Number: string (nullable = true)\n",
      " |-- Credit Card Type: string (nullable = true)\n",
      " |-- CreditCardApprovalCode: string (nullable = true)\n",
      " |-- CustomerID: long (nullable = true)\n",
      " |-- DueDate: string (nullable = true)\n",
      " |-- Freight: double (nullable = true)\n",
      " |-- LineTotal: double (nullable = true)\n",
      " |-- OnlineOrderFlag: string (nullable = true)\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- OrderQty: long (nullable = true)\n",
      " |-- ProductID: long (nullable = true)\n",
      " |-- PurchaseOrderNumber: string (nullable = true)\n",
      " |-- SalesOrderID: long (nullable = true)\n",
      " |-- SalesOrderNumber: string (nullable = true)\n",
      " |-- SalesPersonID: long (nullable = true)\n",
      " |-- ShipBase: double (nullable = true)\n",
      " |-- ShipDate: string (nullable = true)\n",
      " |-- ShipMethod: string (nullable = true)\n",
      " |-- ShipRate: double (nullable = true)\n",
      " |-- Status: long (nullable = true)\n",
      " |-- SubTotal: double (nullable = true)\n",
      " |-- TaxAmt: double (nullable = true)\n",
      " |-- TerritoryID: long (nullable = true)\n",
      " |-- TotalDue: double (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      "\n",
      "Row count: 999\n"
     ]
    }
   ],
   "source": [
    "# BRONZE PHASE \n",
    "\n",
    "# 3.1 Read streamed files \n",
    "df_trans_bronze = spark.readStream \\\n",
    "    .option(\"schemaLocation\", trans_output_bronze) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .json(stream_dir)\n",
    "\n",
    "# 3.2 Write 'bronze' dataframe to parquet file \n",
    "trans_checkpoint_bronze = os.path.join(trans_output_bronze, \"_checkpoint\")\n",
    "trans_bronze_query = (df_trans_bronze\n",
    "                      .withColumn(\"receivedTime\", current_timestamp())\n",
    "                      .withColumn(\"sourceFile\", input_file_name())\n",
    "                      .writeStream.format(\"parquet\")\n",
    "                      .outputMode(\"append\")\n",
    "                      .queryName(\"trans_bronze\")\n",
    "                      .trigger(availableNow=True)\n",
    "                      .option(\"checkpointLocation\", trans_checkpoint_bronze)\n",
    "                      .option(\"compression\", \"snappy\").\n",
    "                      start(trans_output_bronze))\n",
    "\n",
    "# 3.3 Wait for completion of the bronze job\n",
    "trans_bronze_query.awaitTermination()\n",
    "df_trans_bronze.printSchema()\n",
    "\n",
    "# 3.4 Show number of records\n",
    "df_output = spark.read.parquet(trans_output_bronze)\n",
    "print(f\"Row count: {df_output.count()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:05:43.048075Z",
     "start_time": "2025-05-09T21:05:41.540592Z"
    }
   },
   "id": "73c0cb51f92b6ade"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 4 - SILVER PHASE (Integrating \"Cold-Path\" Data with Bronze layer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18204c0839eaafff"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TerritoryID: long (nullable = true)\n",
      " |-- CustomerID: long (nullable = true)\n",
      " |-- ProductID: long (nullable = true)\n",
      " |-- CarrierTrackingNumber: string (nullable = true)\n",
      " |-- Credit Card ExpMonth: long (nullable = true)\n",
      " |-- Credit Card ExpYear: long (nullable = true)\n",
      " |-- Credit Card Number: string (nullable = true)\n",
      " |-- Credit Card Type: string (nullable = true)\n",
      " |-- CreditCardApprovalCode: string (nullable = true)\n",
      " |-- DueDate: date (nullable = true)\n",
      " |-- Freight: double (nullable = true)\n",
      " |-- LineTotal: double (nullable = true)\n",
      " |-- OnlineOrderFlag: string (nullable = true)\n",
      " |-- OrderDate: date (nullable = true)\n",
      " |-- OrderQty: long (nullable = true)\n",
      " |-- PurchaseOrderNumber: string (nullable = true)\n",
      " |-- SalesOrderID: long (nullable = true)\n",
      " |-- SalesOrderNumber: string (nullable = true)\n",
      " |-- SalesPersonID: long (nullable = true)\n",
      " |-- ShipBase: double (nullable = true)\n",
      " |-- ShipDate: date (nullable = true)\n",
      " |-- ShipMethod: string (nullable = true)\n",
      " |-- ShipRate: double (nullable = true)\n",
      " |-- Status: long (nullable = true)\n",
      " |-- SubTotal: double (nullable = true)\n",
      " |-- TaxAmt: double (nullable = true)\n",
      " |-- TotalDue: double (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- receivedTime: timestamp (nullable = true)\n",
      " |-- sourceFile: string (nullable = true)\n",
      " |-- DaysToManufacture: integer (nullable = true)\n",
      " |-- FinishedGoodsFlag: string (nullable = true)\n",
      " |-- ListPrice: double (nullable = true)\n",
      " |-- MakeFlag: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- ProductNumber: string (nullable = true)\n",
      " |-- ReorderPoint: integer (nullable = true)\n",
      " |-- SafetyStockLevel: integer (nullable = true)\n",
      " |-- SellStartDate: string (nullable = true)\n",
      " |-- StandardCost: double (nullable = true)\n",
      " |-- ProductKey: integer (nullable = false)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- LastName: string (nullable = true)\n",
      " |-- EmailAddress: string (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      " |-- AccountNumber: string (nullable = true)\n",
      " |-- CustomerType: string (nullable = true)\n",
      " |-- AddressLine1: string (nullable = true)\n",
      " |-- AddressLine2: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- PostalCode: string (nullable = true)\n",
      " |-- StateProvince: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CustomerKey: integer (nullable = false)\n",
      " |-- CountryRegionCode: string (nullable = true)\n",
      " |-- Group: string (nullable = true)\n",
      " |-- TerritoryName: string (nullable = true)\n",
      " |-- TerritoryKey: integer (nullable = false)\n",
      " |-- FullShipDate: date (nullable = true)\n",
      " |-- FullOrderDate: date (nullable = true)\n",
      " |-- FullDueDate: date (nullable = true)\n",
      "\n",
      "Row count: 1042\n",
      "Query ID: 2ec830c7-3d2e-4977-8fa2-e406d12a0a07\n",
      "Query Name: trans_silver\n",
      "Query Status: {'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "# SILVER PHASE \n",
    "\n",
    "# 4.1 Read bronze output into a stream\n",
    "df_trans_silver = (spark.readStream\n",
    "    .format(\"parquet\")\n",
    "    .load(trans_output_bronze)\n",
    "    # Join dimensional tables \n",
    "    .join(df_dim_products.alias(\"product\"), \"ProductID\")\n",
    "    .join(df_dim_customers.alias(\"customer\"), \"CustomerID\")\n",
    "    .join(df_dim_territories.alias(\"territory\"), \"TerritoryID\")\n",
    "\n",
    "    # Join dimensional date table for each 'Date' in Fact Table\n",
    "    .join(\n",
    "        df_dim_date.withColumn(\"FullShipDate\", col(\"full_date\")).alias(\"dim_date\"),\n",
    "        (col(\"dim_date.FullShipDate\").cast(DateType()) == from_unixtime(col(\"ShipDate\") / 1000).cast(DateType())), \n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        df_dim_date.withColumn(\"FullOrderDate\", col(\"full_date\")).alias(\"dim_date2\"),\n",
    "        (col(\"dim_date2.FullOrderDate\").cast(DateType()) == from_unixtime(col(\"OrderDate\") / 1000).cast(DateType())),\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        df_dim_date.withColumn(\"FullDueDate\", col(\"full_date\")).alias(\"dim_date3\"),\n",
    "        (col(\"dim_date3.FullDueDate\").cast(DateType()) == from_unixtime(col(\"DueDate\") / 1000).cast(DateType())),\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "    # Ensure correct typing of dates\n",
    "    .withColumn(\"DueDate\", col(\"DueDate\").cast(DateType()))\n",
    "    .withColumn(\"OrderDate\", col(\"OrderDate\").cast(DateType()))\n",
    "    .withColumn(\"ShipDate\", col(\"ShipDate\").cast(DateType()))\n",
    "\n",
    "    # Drop any redundant data (from joined tables)\n",
    "    .drop(\"date_name\")\n",
    "    .drop(\"date_key\")\n",
    "    .drop(\"full_date\")              \n",
    ")\n",
    "\n",
    "# 4.2 Write 'silver' dataframe to parquet file \n",
    "trans_checkpoint_silver = os.path.join(trans_output_silver, \"_checkpoint\")\n",
    "trans_silver_query = (\n",
    "    df_trans_silver.writeStream\n",
    "    .format(\"parquet\")\n",
    "    .outputMode(\"append\")\n",
    "    .queryName(\"trans_silver\")\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", trans_checkpoint_silver)\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .start(trans_output_silver)\n",
    ")\n",
    "\n",
    "# 4.3 Wait for completion of the 'silver' job\n",
    "trans_silver_query.awaitTermination()\n",
    "df_trans_silver.printSchema()\n",
    "\n",
    "# 4.4 Show number of records\n",
    "df_output = spark.read.parquet(trans_output_silver)\n",
    "print(f\"Row count: {df_output.count()}\")\n",
    "print(f\"Query ID: {trans_silver_query.id}\")\n",
    "print(f\"Query Name: {trans_silver_query.name}\")\n",
    "print(f\"Query Status: {trans_silver_query.status}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:05:48.074990Z",
     "start_time": "2025-05-09T21:05:45.400772Z"
    }
   },
   "id": "fd097ec5065468d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 5 - GOLD PHASE (PERFORM AGGREGATIONS/BUSINESS LOGIC)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1efc8ce12516978c"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stream has processed 1 batchs\n"
     ]
    }
   ],
   "source": [
    "# GOLD PHASE\n",
    "\n",
    "# 5.1 Example business-level logic:\n",
    "# - Get the first and last name of the top 10 customers that have spent the most in the US \n",
    "# territory. If the data warehouse does not contain a first and last name for the customer, do not\n",
    "# include this value (as not all customers have first and last name data from the adventureworks\n",
    "# database).\n",
    "\n",
    "# 5.2 Write query to get business-level logic\n",
    "from pyspark.sql.functions import sum as _sum, col\n",
    "df_trans_gold = (\n",
    "    spark.readStream\n",
    "    .format(\"parquet\")\n",
    "    .load(trans_output_silver)\n",
    "    .filter(col(\"CountryRegionCode\") == 'US')\n",
    "    .filter(col(\"FirstName\").isNotNull())\n",
    "    .filter(col(\"LastName\").isNotNull())\n",
    "    .groupby(col(\"CustomerID\"), col(\"FirstName\"), col(\"LastName\"))\n",
    "    .agg(_sum(col(\"TotalDue\")).alias(\"TotalSpent\"))\n",
    ")\n",
    "\n",
    "# 5.3 Output query to memory\n",
    "trans_gold_query = (\n",
    "    df_trans_gold.writeStream\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    "    .queryName(\"businessQuery\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# 5.4 Wait until all of the query is in memory\n",
    "wait_until_stream_is_ready(trans_gold_query, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:05:58.888721Z",
     "start_time": "2025-05-09T21:05:53.669883Z"
    }
   },
   "id": "c094a000af9aa85"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerID: long (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- LastName: string (nullable = true)\n",
      " |-- TotalSpent: double (nullable = true)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   CustomerID   FirstName  LastName  TotalSpent\n0       27606    Courtney   Edwards   3953.9884\n1       27577     Patrick      Cook   3953.9884\n2       27668       Kevin  Gonzalez   3953.9884\n3       27625  Alexandria    Hughes   3953.9884\n4       27611        Jack   Edwards   3953.9884\n5       27605      Miguel  Martinez   3953.9884\n6       27651      Jeremy    Murphy   3953.9884\n7       27621      Edward     Brown   3953.9884\n8       27646     Jackson        Li   3953.9884\n9       27636       Paige    Murphy   3953.9884",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CustomerID</th>\n      <th>FirstName</th>\n      <th>LastName</th>\n      <th>TotalSpent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>27606</td>\n      <td>Courtney</td>\n      <td>Edwards</td>\n      <td>3953.9884</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>27577</td>\n      <td>Patrick</td>\n      <td>Cook</td>\n      <td>3953.9884</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>27668</td>\n      <td>Kevin</td>\n      <td>Gonzalez</td>\n      <td>3953.9884</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27625</td>\n      <td>Alexandria</td>\n      <td>Hughes</td>\n      <td>3953.9884</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>27611</td>\n      <td>Jack</td>\n      <td>Edwards</td>\n      <td>3953.9884</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>27605</td>\n      <td>Miguel</td>\n      <td>Martinez</td>\n      <td>3953.9884</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>27651</td>\n      <td>Jeremy</td>\n      <td>Murphy</td>\n      <td>3953.9884</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>27621</td>\n      <td>Edward</td>\n      <td>Brown</td>\n      <td>3953.9884</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>27646</td>\n      <td>Jackson</td>\n      <td>Li</td>\n      <td>3953.9884</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>27636</td>\n      <td>Paige</td>\n      <td>Murphy</td>\n      <td>3953.9884</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.5 Get business query data after stream finished\n",
    "df_business_query = spark.sql(\"SELECT * FROM businessQuery\")\n",
    "df_business_query.printSchema()\n",
    "\n",
    "# 5.6 Write business query as table\n",
    "df_business_query.write.saveAsTable(f\"{dest_database}.fact_business_query\", mode=\"overwrite\")\n",
    "\n",
    "# 5.7 Show result of business query\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.fact_business_query ORDER BY TotalSpent DESC LIMIT 10\").toPandas()\n",
    "\n",
    "# Why the TotalSpent column is the same:\n",
    "# Yes, the total spent for the top-spending individuals are the same. I used a subset of all the fact sales data from the adventureworks database (999 total records in 3 .json files in the /data/adventureworks/streaming/ directory). This means I do not use all the fact sales data as I meant to use these files as an example of real-time streamed data in this directory for the system to process. This is why the output from this table is different from the midterm, despite using similar schemas (they don't use exactly the same data). "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T21:06:18.756483Z",
     "start_time": "2025-05-09T21:06:18.536490Z"
    }
   },
   "id": "46ac54561dd9a37"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T18:36:15.045183Z",
     "start_time": "2025-05-09T18:35:40.022584Z"
    }
   },
   "id": "6bfa0e7737a71daf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
